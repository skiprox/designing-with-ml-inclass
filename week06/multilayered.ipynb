{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6\n",
    "\n",
    "Multi layered perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring in tensorflow and numpy\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "mnist = tf.keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break apart the data, to have stuff to test against the training\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136\n",
      "  175  26 166 255 247 127   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253\n",
      "  225 172 253 242 195  64   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251\n",
      "   93  82  82  56  39   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119\n",
      "   25   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253\n",
      "  150  27   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252\n",
      "  253 187   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249\n",
      "  253 249  64   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
      "  253 207   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253\n",
      "  250 182   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201\n",
      "   78   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# Have a look at the first item in x_train and y_train\n",
    "print(x_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to flatten everything\n",
    "x_train_flat = np.vstack([img.reshape(-1,) for img in x_train])\n",
    "x_test_flat = np.vstack([img.reshape(-1,) for img in x_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   3  18  18  18 126 136 175  26 166 255\n",
      " 247 127   0   0   0   0   0   0   0   0   0   0   0   0  30  36  94 154\n",
      " 170 253 253 253 253 253 225 172 253 242 195  64   0   0   0   0   0   0\n",
      "   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251  93  82\n",
      "  82  56  39   0   0   0   0   0   0   0   0   0   0   0   0  18 219 253\n",
      " 253 253 253 253 198 182 247 241   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  14   1 154 253  90   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0  11 190 253  70   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  35 241\n",
      " 225 160 108   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0  81 240 253 253 119  25   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0  45 186 253 253 150  27   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252 253 187\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0 249 253 249  64   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
      " 253 207   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0  39 148 229 253 253 253 250 182   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253\n",
      " 253 201  78   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0  23  66 213 253 253 253 253 198  81   2   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  18 171 219 253 253 253 253 195\n",
      "  80   9   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  55 172 226 253 253 253 253 244 133  11   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0 136 253 253 253 212 135 132  16\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# print this flattened bad boy out :)\n",
    "print(x_train_flat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_onehot = tf.keras.utils.to_categorical(y_train)\n",
    "\n",
    "y_train_onehot[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Dense(\n",
    "    units=50,\n",
    "    input_dim=x_train_flat.shape[1],\n",
    "    activation='sigmoid'\n",
    "))\n",
    "\n",
    "model.add(keras.layers.Dense(\n",
    "    units=50,\n",
    "    input_dim=50,\n",
    "    activation='sigmoid'\n",
    "))\n",
    "\n",
    "model.add(keras.layers.Dense(\n",
    "    units=y_train_onehot.shape[1],\n",
    "    input_dim=50,\n",
    "    activation='softmax'\n",
    "))\n",
    "\n",
    "sgd_optimizer = keras.optimizers.SGD(\n",
    "    lr=0.001,\n",
    "    decay=1e-7,\n",
    "    momentum=0.9\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=sgd_optimizer,\n",
    "    loss='categorical_crossentropy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 50)                39250     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 42,310\n",
      "Trainable params: 42,310\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/50\n",
      "54000/54000 [==============================] - 3s 58us/sample - loss: 2.0006 - val_loss: 1.6881\n",
      "Epoch 2/50\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 1.4315 - val_loss: 1.1519\n",
      "Epoch 3/50\n",
      "54000/54000 [==============================] - 2s 40us/sample - loss: 1.0059 - val_loss: 0.8147\n",
      "Epoch 4/50\n",
      "54000/54000 [==============================] - 2s 42us/sample - loss: 0.7563 - val_loss: 0.6238\n",
      "Epoch 5/50\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.6143 - val_loss: 0.5180\n",
      "Epoch 6/50\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.5261 - val_loss: 0.4384\n",
      "Epoch 7/50\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.4686 - val_loss: 0.3971\n",
      "Epoch 8/50\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.4261 - val_loss: 0.3607\n",
      "Epoch 9/50\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.3951 - val_loss: 0.3315\n",
      "Epoch 10/50\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.3709 - val_loss: 0.3315\n",
      "Epoch 11/50\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.3601 - val_loss: 0.3026\n",
      "Epoch 12/50\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.3432 - val_loss: 0.2936\n",
      "Epoch 13/50\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.3276 - val_loss: 0.2883\n",
      "Epoch 14/50\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.3204 - val_loss: 0.2754\n",
      "Epoch 15/50\n",
      "54000/54000 [==============================] - 2s 38us/sample - loss: 0.3052 - val_loss: 0.2645\n",
      "Epoch 16/50\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.3010 - val_loss: 0.2621\n",
      "Epoch 17/50\n",
      "54000/54000 [==============================] - 2s 43us/sample - loss: 0.2908 - val_loss: 0.2544\n",
      "Epoch 18/50\n",
      "54000/54000 [==============================] - 2s 39us/sample - loss: 0.2890 - val_loss: 0.2520\n",
      "Epoch 19/50\n",
      "54000/54000 [==============================] - 2s 40us/sample - loss: 0.2789 - val_loss: 0.2348\n",
      "Epoch 20/50\n",
      "54000/54000 [==============================] - 2s 41us/sample - loss: 0.2684 - val_loss: 0.2302\n",
      "Epoch 21/50\n",
      "54000/54000 [==============================] - 2s 43us/sample - loss: 0.2692 - val_loss: 0.2395\n",
      "Epoch 22/50\n",
      "54000/54000 [==============================] - 2s 43us/sample - loss: 0.2640 - val_loss: 0.2310\n",
      "Epoch 23/50\n",
      "54000/54000 [==============================] - 2s 46us/sample - loss: 0.2595 - val_loss: 0.2313\n",
      "Epoch 24/50\n",
      "54000/54000 [==============================] - 2s 43us/sample - loss: 0.2603 - val_loss: 0.2247\n",
      "Epoch 25/50\n",
      "54000/54000 [==============================] - 3s 47us/sample - loss: 0.2562 - val_loss: 0.2170\n",
      "Epoch 26/50\n",
      "54000/54000 [==============================] - 3s 52us/sample - loss: 0.2476 - val_loss: 0.2104\n",
      "Epoch 27/50\n",
      "54000/54000 [==============================] - 2s 41us/sample - loss: 0.2463 - val_loss: 0.2150\n",
      "Epoch 28/50\n",
      "54000/54000 [==============================] - 2s 40us/sample - loss: 0.2401 - val_loss: 0.2170\n",
      "Epoch 29/50\n",
      "54000/54000 [==============================] - 2s 40us/sample - loss: 0.2377 - val_loss: 0.2064\n",
      "Epoch 30/50\n",
      "54000/54000 [==============================] - 2s 42us/sample - loss: 0.2322 - val_loss: 0.1933\n",
      "Epoch 31/50\n",
      "54000/54000 [==============================] - 3s 50us/sample - loss: 0.2272 - val_loss: 0.1984\n",
      "Epoch 32/50\n",
      "54000/54000 [==============================] - 2s 46us/sample - loss: 0.2222 - val_loss: 0.1904\n",
      "Epoch 33/50\n",
      "54000/54000 [==============================] - 2s 40us/sample - loss: 0.2249 - val_loss: 0.1945\n",
      "Epoch 34/50\n",
      "54000/54000 [==============================] - 2s 40us/sample - loss: 0.2220 - val_loss: 0.1931\n",
      "Epoch 35/50\n",
      "54000/54000 [==============================] - 2s 43us/sample - loss: 0.2125 - val_loss: 0.1936\n",
      "Epoch 36/50\n",
      "54000/54000 [==============================] - 2s 40us/sample - loss: 0.2114 - val_loss: 0.1908\n",
      "Epoch 37/50\n",
      "54000/54000 [==============================] - 2s 42us/sample - loss: 0.2147 - val_loss: 0.1849\n",
      "Epoch 38/50\n",
      "54000/54000 [==============================] - 2s 45us/sample - loss: 0.2102 - val_loss: 0.1848\n",
      "Epoch 39/50\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.2132 - val_loss: 0.1831\n",
      "Epoch 40/50\n",
      "54000/54000 [==============================] - 2s 42us/sample - loss: 0.2066 - val_loss: 0.1761\n",
      "Epoch 41/50\n",
      "54000/54000 [==============================] - 2s 40us/sample - loss: 0.2075 - val_loss: 0.1891\n",
      "Epoch 42/50\n",
      "54000/54000 [==============================] - 2s 42us/sample - loss: 0.2029 - val_loss: 0.1791\n",
      "Epoch 43/50\n",
      "54000/54000 [==============================] - 2s 42us/sample - loss: 0.2044 - val_loss: 0.1776\n",
      "Epoch 44/50\n",
      "54000/54000 [==============================] - 2s 43us/sample - loss: 0.2053 - val_loss: 0.1792\n",
      "Epoch 45/50\n",
      "54000/54000 [==============================] - 3s 47us/sample - loss: 0.1997 - val_loss: 0.1725\n",
      "Epoch 46/50\n",
      "54000/54000 [==============================] - 2s 41us/sample - loss: 0.1957 - val_loss: 0.1725\n",
      "Epoch 47/50\n",
      "54000/54000 [==============================] - 2s 43us/sample - loss: 0.1941 - val_loss: 0.1733\n",
      "Epoch 48/50\n",
      "54000/54000 [==============================] - 2s 41us/sample - loss: 0.1970 - val_loss: 0.1747\n",
      "Epoch 49/50\n",
      "54000/54000 [==============================] - 2s 41us/sample - loss: 0.1877 - val_loss: 0.1617\n",
      "Epoch 50/50\n",
      "54000/54000 [==============================] - 2s 41us/sample - loss: 0.1900 - val_loss: 0.1773\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x_train_flat, y_train_onehot,\n",
    "    batch_size=64, epochs=50,\n",
    "    verbose=1, validation_split=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [2.0005874100791083,\n",
       "  1.4314662099061188,\n",
       "  1.005932243488453,\n",
       "  0.7563355683220757,\n",
       "  0.6142530125688623,\n",
       "  0.5260969237133308,\n",
       "  0.4686174628381376,\n",
       "  0.42607604755295647,\n",
       "  0.39512892022839297,\n",
       "  0.3709161547554864,\n",
       "  0.3600891110543851,\n",
       "  0.34318168983636077,\n",
       "  0.3276219859741352,\n",
       "  0.32043571737077503,\n",
       "  0.305168840624668,\n",
       "  0.30100486447193003,\n",
       "  0.2907800928884082,\n",
       "  0.2890208961257228,\n",
       "  0.2789106012097112,\n",
       "  0.2684357781233611,\n",
       "  0.26921857810903477,\n",
       "  0.26402828529587496,\n",
       "  0.25946984548038904,\n",
       "  0.26031255513208884,\n",
       "  0.2562227491449427,\n",
       "  0.24758758532117914,\n",
       "  0.24628459442544867,\n",
       "  0.2400737079249488,\n",
       "  0.23773709504251128,\n",
       "  0.2322345255723706,\n",
       "  0.22720695299572416,\n",
       "  0.22216196819146475,\n",
       "  0.22485607407269653,\n",
       "  0.2220181587625433,\n",
       "  0.21245224236779742,\n",
       "  0.2114051930749858,\n",
       "  0.21470114071280869,\n",
       "  0.21020304471916623,\n",
       "  0.21316777561770545,\n",
       "  0.20657154004662126,\n",
       "  0.2074646602736579,\n",
       "  0.20292669974874566,\n",
       "  0.20440292647591343,\n",
       "  0.20531370590351247,\n",
       "  0.19965535231431325,\n",
       "  0.19571247973706987,\n",
       "  0.19406246191042442,\n",
       "  0.19703537391512482,\n",
       "  0.1877200860844718,\n",
       "  0.18998268304930793],\n",
       " 'val_loss': [1.6881321427027385,\n",
       "  1.1518668018976848,\n",
       "  0.8147357099850973,\n",
       "  0.6238066906929016,\n",
       "  0.518037237405777,\n",
       "  0.43844957669576007,\n",
       "  0.39707230345408123,\n",
       "  0.36074719615777334,\n",
       "  0.3315400190750758,\n",
       "  0.3315396766662598,\n",
       "  0.30258898401260376,\n",
       "  0.2936188986698786,\n",
       "  0.2882680013974508,\n",
       "  0.2753786060015361,\n",
       "  0.26452402271827063,\n",
       "  0.2621191179354986,\n",
       "  0.2543817274173101,\n",
       "  0.25200873984893163,\n",
       "  0.2347760291894277,\n",
       "  0.2301903398434321,\n",
       "  0.2394648541212082,\n",
       "  0.2309932125409444,\n",
       "  0.23130678365627924,\n",
       "  0.22467489484945932,\n",
       "  0.21697963815927507,\n",
       "  0.21042171976963678,\n",
       "  0.21503653728961944,\n",
       "  0.21697818620999654,\n",
       "  0.20643952910105387,\n",
       "  0.1933315968811512,\n",
       "  0.19843767505884172,\n",
       "  0.19037932846943537,\n",
       "  0.19453234297037125,\n",
       "  0.19306124638517697,\n",
       "  0.19355871871113778,\n",
       "  0.19084390987952551,\n",
       "  0.18487406114737193,\n",
       "  0.18476407596468924,\n",
       "  0.18305817849437395,\n",
       "  0.17605663075049718,\n",
       "  0.18905461859703063,\n",
       "  0.179142551779747,\n",
       "  0.17760991142193477,\n",
       "  0.17921323734521866,\n",
       "  0.1724762609998385,\n",
       "  0.17252160075306894,\n",
       "  0.17326748780409496,\n",
       "  0.17469948302706081,\n",
       "  0.16169887947042783,\n",
       "  0.17728999397158623]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
